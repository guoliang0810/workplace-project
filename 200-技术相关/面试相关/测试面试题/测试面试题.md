## 一、Web 测试与 App 测试的区别

  

1. **功能测试层面**：
    
    1. Web 和 App 测试在流程上相似，但载体不同。Web 项目为 B/S 架构，基于浏览器，服务端更新后客户端同步更新；App 项目为 C/S 架构，服务器更新后，客户端需手动更新。
        
2. **性能测试方面**：
    
    1. Web 测试关注响应时间、内存、CPU、吞吐量、并发数。App 测试除响应时间、内存、CPU 外，还需测试消耗电量、流量。
        
3. **兼容性方面**：
    
    1. Web 主要考虑多种浏览器及版本。App 主要考虑手机品牌、型号、尺寸、分辨率、版本。
        
4. **测试工具方面**：
    
    1. 自动化测试，Web 一般用 Selenium，App 用 Appium。性能测试，Web 一般用 LoadRunner，App 用 Jmeter。App 测试还需关注干扰测试（来电、短信、通话、关机、重启）、不同网络下的测试（网络切换、无网）、安装、卸载、更新测试。
        

  

## 二、有效评估测试用例质量的方法

  

-  单个测试用例应具备可读性、可执行性、可重复使用性，且用例中的数据及步骤应及时更新。
    
- 总体测试用例需考虑覆盖率、失效率、缺陷查找率。可根据市面上的数据或公司规定的评估数据，从以下指标判断质量水平：
    1. 覆盖率：测试用例对需求的覆盖程度。
        
    2. 失效率：执行测试用例发现缺陷的比例。
        
    3. 缺陷查找率：测试用例能够发现缺陷的能力。
        

  

## 三、制定测试计划的步骤


-  **测试背景**：包括项目内容、人员配备、项目模块等。
    
- **测试目标**：如逻辑功能达标率、界面测试与产品原型图覆盖率、性能测试内容等。
    
-  **测试范围**：
    
    1. 测试分类：包括单元测试、集成测试、系统测试、回归测试、随机测试、兼容性测试等。
        
    2. 测试输出文档：如测试用例、bug 报告、测试报告等。
        
- **测试工具**：功能测试工具、性能测试工具、自动化测试工具等。
    
- **人员安排**：明确各测试人员的任务分工，包括模块安排、测试分类安排。
    
-  **测试进度**：确定各阶段的时间节点，如测试用例编写、评审、各类测试的开始与结束时间，以及特定工程师完成特定模块测试的时间。
    

  

## 四、在项目中保证软件质量的方法

  

13. **产品方面**：保证迭代过程中的产品逻辑，对可能的兼容、升级问题做出预判并给出方案。
    
14. **设计方面**：在满足产品需求的同时，保证设计的延续性。
    
15. **开发方面**：注重产品细节，技术方案选择严谨，考虑兼容、性能，开发完成后充分自测，严格遵循开发规范操作。
    
16. **测试方面**：验证产品逻辑，从用户角度对交互设计进行系统验证，运用技术手段保证测试质量。
    

  

## 五、定位 App 端与服务端问题的方法

  

17. 抓包分析：通过对客户端抓包，分析服务端返回数据是否符合预期。若服务端数据正确，则问题可能在客户端；若数据不正确，则可能是服务端问题。
    
18. 日志分析：查看客户端或服务端日志，分析有无异常信息，以确定问题原因。
    

  

## 六、Bug 的生命周期

  

发现 Bug 后，测试人员提交，开发人员验证。若开发认为是缺陷，则进行修改；若认为不是，开发与测试进行讨论。若讨论后确定为缺陷，开发修改，修改后测试人员重新验证。若验证通过，关闭 Bug；若仍存在问题，重新打开 Bug。若确定不改，在测试报告中记录。

  

## 七、当开发人员说不是 Bug 时的处理方法

  

19. 若需求未确定，找产品经理确认是否需要改动，商量后决定。
    
20. 若开发认为不可能发生而无需修改，测试人员应说出 Bug 的依据，分析不良后果。若无法达成一致，可与开发经理和测试经理确认。若确定不改，在测试报告中记录。
    

  

## 八、处理概率性 Bug 的方法

  

21. 明确该类 Bug 也需提单，描述操作环境、步骤、数据并提供必要日志，可备注可能原因。
    
22. 运用排除法、错误推测找规律，必要时与开发人员、项目经理一起定位分析讨论。
    
23. 若最终未解决，在测试报告中体现，并分析可能造成的影响，权衡是否可遗留。
    

  

## 九、测试纸杯的方法

  

24. **功能度**：装水测试是否漏水，能否喝到水。
    
25. **安全性**：检测是否有毒或细菌。
    
26. **可靠性**：从不同高度落下，测试损坏程度。
    
27. **可移植性**：在不同地方、温度等环境下测试是否正常使用。
    
28. **兼容性**：测试能否容纳果汁、白水、酒精、汽油等。
    
29. **易用性**：测试是否烫手、有无防滑措施、是否方便饮用。
    
30. **用户文档**：检查使用手册对杯子用法、限制、使用条件等的描述。
    
31. **疲劳测试**：盛水或汽油放置 24 小时，检查泄漏时间和情况。
    
32. **压力测试**：用针加重量，测试压强多大时会穿透。
    

  

## 十、新用户注册后名字不完整的测试思路

  

33. **思路一**：
    
    1. 重新发送注册请求，用抓包工具抓取请求信息，检查前端请求数据与发送数据是否一致。若不一致，在浏览器开发者工具中使用 console 对话框初步调试，调用元素查看是否显示正常。
        
    2. 若前端请求数据正常，使用 Linux 命令查看后端服务器打印的日志，看是否有报错信息。
        
    3. 用 SQL 语句在数据库查看该用户的数据入库情况，判断是否因数据库导致入库信息不完整。
        
34. **思路二**：
    
    1. 复现 Bug，若能复现，先检查数据库中名称字段是否完整。
        
    2. 若不完整，对注册接口抓包，检查前端传给后端的数据是否完整。若不完整，提出前端 Bug；若完整，提出后端 Bug。
        
    3. 若数据库中名称字段完整，对显示用户名接口抓包，检查后端传给前端时名称是否完整。若完整，提出前端 Bug；若不完整，提出后端 Bug。
        
    4. 若不能复现，做好记录，优先级调用后跟踪，可进一步考虑前端错误的位数限制、设备兼容性、查看后台日志等因素。
        

  

## 十一、测试流程

  

35. 需求评审和分析。
    
36. 制定测试计划。
    
37. 根据需求文档编写测试用例。
    
38. 测试用例评审。
    
39. 提测后执行冒烟测试。
    
40. 执行第一轮测试，找 Bug。
    
41. 执行回归测试，验证 Bug。
    
42. 执行第二轮测试。
    
43. 部署项目到预生产环境。
    
44. 预生产环境测试。
    
45. 发测试报告。
    
46. 项目上线。
    

  

## 十二、提交高质量缺陷跟踪单的方法

  

47. 标题简洁明了，步骤条理清晰，让别人一看就懂。
    
48. 考虑缺陷的完备性，包括缺陷等级、所属功能模块、版本、复现步骤、预期结果、实际结果、产生原因、日志截图等。
    

  

## 十三、Bug 优先级和严重程度的划分

  

49. 严重：需立即解决，如死机、进程无响应、崩溃。
    
50. 高：软件主要功能错误或引起数据丢失。
    
51. 中：影响软件功能和性能的一般缺陷。
    
52. 低：对软件质量影响轻微，多为建议性或 UI 层级问题。
    

  

## 十四、做好测试用例设计工作的关键

  

53. 熟悉业务需求和用户使用场景。
    
54. 了解本次需求对其他系统的影响。
    
55. 了解开发技术实现和数据库设计。
    
56. 从不同维度编写测试用例，如功能、性能、安全、兼容等。
    

  

## 十五、开展项目测试的方法

  

57. 分析需求：查找需求说明、项目设计等相关文档，分析需求。
    
58. 制定计划：确定测试范围和策略，制定测试计划。
    
59. 设计用例：包括功能、兼容、性能、安全等方面的测试用例。
    
60. 执行测试：开展测试执行。
    
61. 回归测试及发送报告：进行回归测试，发送测试报告。
    

  

## 十六、Bug 的生命周期

  

New：新发现的 Bug，指定给对应的开发。

Open：开发确认 Bug，认为需要修改。

Fixed：开发人员修改后标识为已修复状态，等待测试人员回归测试验证。

Rejected：开发认为不是 Bug 则拒绝修改。

Delay：认为暂时不需要修改或不能修改则延后修改，并给出理由。

Closed：修改状态的 Bug 经测试人员回归测试验证通过则关闭。

Reopen：若经验证 Bug 仍然存在，则重新打开，开发人员重新修改。

Later：延期修改（下一个版本修复）。

  

## 十七、黑盒测试与白盒测试的区别

  

62. 黑盒测试：把系统当成黑盒子，不了解内部细节，只关注输入和输出。通过手动输入不同数据，验证输出是否符合预期。
    
63. 白盒测试：了解系统内部实现细节，针对函数进行测试。需写测试代码调用函数，传入不同参数，测试函数返回值是否符合预期。
    

  

## 十八、测试报告的内容

  

测试范围、测试时间、参与人员、测试策略、Bug 数量、上线风险、遗留问题、测试是否通过。

  

## 十九、提高用例覆盖率、减少漏测的方法

  

64. 根据需求文档编写用例，确保每条需求都有对应用例覆盖。
    
65. 充分理解业务，挖掘隐形需求并编写用例。
    
66. 考虑异常场景和数据。
    
67. 从多个维度测试软件，如功能、性能、安全等。
    
68. 站在用户角度思考问题，模拟用户使用场景。
    
69. 组织用例评审。
    

  

## 二十、确定 Bug 的方法

  

70. 查看需求文档，是否有明确要求。
    
71. 考虑问题是否违反正常人行为习惯或行业通用规范。
    
72. 找产品经理或开发人员沟通确定。
    
73. 对于无法达成一致的问题，组织相关人员开会决定。
    

  

## 二十一、QQ 阅读书架功能

  

74. 打开 QQ 阅读后，默认显示图书书架，方便阅读已下载或收藏的图书。点击“全部”按钮可分类查看图书。
    
75. 可新建分类。
    
76. 点击左上角“编辑”按钮，可对书架中的图书进行管理，如删除、移动分类。
    
77. 选中图书后点击“移动到”按钮，可将图书移动到指定分类。
    

  

## 二十二、微信朋友圈点赞功能

  

78. **功能**：
    
    1. 点赞和取消点赞功能。
        
    2. 点赞是否按时间顺序显示，头像显示是否正确。
        
    3. 是否支持多次点赞。
        
    4. 点赞人数是否有上限。
        
    5. 点赞后共同好友是否可见。
        
    6. 点赞状态能否正常更新，是否提醒。
        
    7. 赞后共同好友的点赞和评论是否提醒。
        
79. **UI**：
    
    1. 点赞是否正确显示昵称或备注。
        
    2. 点赞显示是否正确，一行显示几个。
        
    3. 最多显示几行或最多展示多少点赞人的信息。
        
80. **网络**：
    
    1. 弱网络情况下点赞能否实时更新。
        
    2. 点赞时有短信或电话进来，能否显示点赞情况。
        
    3. 网速快慢对其影响。
        
    4. 断网对其影响。
        
81. **安全**：
    
    1. 点赞的人是否在可见分组里。
        
    2. 未登录时能否查看点赞信息。
        
82. **兼容性**：不同手机界面显示如何。
    
83. **扩展性**：能否在点赞后进行评论。
    

  

## 二十三、临近上线发现 Bug 的处理方法

  

84. 若上午验收完成，下午发现 Bug，且晚上 20:00 上线：
    
    1. 找开发确定 Bug 严重程度及修复时间。若能很快修复且不影响上线，将 Bug 入管理平台，让开发修复。
        
    2. 若上线前无法完成，找产品和开发开会。若能延期，视 Bug 严重程度决定是否延期上线；若不能延期，严重 Bug 考虑功能是否可砍掉，不严重则下个版本解决。
        

  

## 二十四、产品上线后发现 Bug 的处理方法

  

85. 若问题轻微且不会对公司造成损失，下个版本解决。
    
86. 若严重或对公司造成损失：
    
    1. App 热修复或后端/HTML 修复后重启服务器。
        

  

## 二十五、Bug 不能复现的处理方法

  

87. 考虑环境问题，尝试还原原来的环境。
    
88. 尽量回想复现步骤，按步骤组合尝试复现。
    
89. 与开发人员配合，让开发检查代码，看能否找出问题。
    
90. 保留发生 Bug 时的 log，附加到提交的 Bug 中，希望从 log 中找到线索。
    
91. 查看代码，可能是代码变更引起的 Bug。
    
92. 遇到问题就要提，在提交的 Bug 描述中加上复现概率，交给开发，开发会根据复现概率调整 Bug 优先级。
    

  

## 二十六、B/S 和 C/S 架构的区别

  

93. CS（Client/Server）架构：
    
    1. 特点：交互性强、安全存取模式、网络通信量低、响应速度快、利于处理大量数据。
        
94. BS（Browser/Server）架构：
    
    1. 安装维护一个服务器，客户端采用浏览器运行软件。特点：分布性强、维护方便、开发简单、共享性强、总体拥有成本低。
        

  

## 二十七、电商下单的业务分析

  

（此处可根据具体分析内容进行优化描述）

  

## 二十八、静态测试、动态测试、黑盒测试、白盒测试、α测试、β测试的简述

  

95. 静态测试：不运行程序，寻找程序代码中可能存在的错误或评估程序代码。
    
96. 动态测试：实际运行被测程序，输入测试实例，检查运行结果与预期结果的差异，判定执行结果是否符合要求，检验程序的正确性、可靠性和有效性，分析系统运行效率和健壮性等性能。
    
97. 黑盒测试：确认软件功能的正确性和可操作性，把被测试程序当作黑盒子，不考虑内部结构，根据输入和输出关系及软件规格说明书确定测试用例和推断测试结果的正确性。
    
98. 白盒测试：根据软件内部逻辑结构分析进行测试，基于代码，测试人员通过阅读程序代码或使用开发工具单步调试判断软件质量，一般由项目经理在程序员开发中实现。
    
99. α测试：由一个用户在开发环境下进行的测试，或公司内部用户在模拟实际操作环境下进行的受控测试，不能由程序员或测试员完成。
    
100. β测试：软件的多个用户在实际使用环境下进行的测试，开发者通常不在测试现场，不能由程序员或测试员完成。
    

  

## 二十九、Bug 特殊状态的处理方法

  

101. **延期**：
    
    1. 确认 Bug 是否紧急。
        
    2. 回报测试领导，确认是否可延期。
        
    3. 若不能延期，催促修改。
        
    4. 修复完成后进行验证。
        
102. **拒绝**：
    
    1. 找需求、原型设计等证据。
        
    2. 拿着证据与开发沟通。
        
    3. 若开发不承认，找测试领导沟通。
        
    4. 测试经理和开发经理确认，决定是否修改。
        
    5. 若确认不是 Bug 不需要修复，将状态改为 abandon（遗弃）。
        
    6. 若需要修改，修改完成后检查。
        
103. **重复**：
    
    1. 查看缺陷报告，确认是否重复。
        
    2. 若不重复，与开发沟通，正常修改。
        
    3. 若重复，将 Bug 的状态修改为 abandon（遗弃）。
        

  

## 三十、Bug 跟踪的方法

  

使用禅道进行 Bug 跟踪，通过 Bug 的状态进行管理：

104. 测试人员执行用例，发现 Bug，在禅道中新建 Bug。
    
105. 开发人员打开 Bug 进行确认，判断是否重复。若重复，标记重复，测试人员确认；若重复则将 Bug 状态设置为遗弃。
    
106. 开发确认若不是重复，判断是否拒绝。若拒绝，测试人员检查，经领导讨论决定是否需要修改。若不需要修改，将状态设置为遗弃。
    
107. 若需要修改，开发人员决定是否延期处理。若需要延期，测试人员确认；若不需要延期，开发人员修改完成后，测试人员进行核验。
    
108. 核验通过，将状态设置为关闭；若不通过，将 Bug 状态设置为重新打开，开发继续修改。
    

  

## 三十一、合理安排测试时间及测试轮次

  

109. 测试计划时间安排遵循趋势收敛原则，越到后面周期越短，问题应越少。
    
110. 测试执行原则是尽可能将问题暴露在前面，保证测试时间呈收敛势。
    
111. 一般经过 4 轮测试（3+1）：
    
    1. 第一轮：全面覆盖所有用例，进行系统测试。
        
    2. 第二轮：基本功能全覆盖，回归 Bug，对缺陷多的模块功能全覆盖。
        
    3. 第三轮：回归 Bug+基本功能全覆盖（执行一级用例）。
        
    4. 第四轮：备用时间，若第三轮有未关闭的 Bug，再加一轮用于回归 Bug。
        

  